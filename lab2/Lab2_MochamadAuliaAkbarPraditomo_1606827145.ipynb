{"cells":[{"metadata":{},"cell_type":"markdown","source":"Mochamad Aulia Akbar Praditomo\n1606827145\nDSA - B"},{"metadata":{},"cell_type":"markdown","source":"**Bagian 1 data hr_data.csv**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nhr.hist('satisfaction_level')\nhr.hist('last_evaluation')\nhr.hist('number_project')\nhr.hist('average_montly_hours')\nhr.hist('time_spend_company')\nhr.hist('Work_accident')\nhr.hist('left')\nhr.hist('promotion_last_5years')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nhr.boxplot(column=['satisfaction_level','last_evaluation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nhr.boxplot(column=['Work_accident','left','promotion_last_5years'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nhr.boxplot(column=['number_project','time_spend_company'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nhr.boxplot(column='average_montly_hours')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nfeature_cols = ['satisfaction_level','last_evaluation','number_project',\n                'average_montly_hours','time_spend_company','Work_accident',\n                'promotion_last_5years','sales','salary']\nX = hr[feature_cols]\ny = hr.left\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nprint(hr.isnull().any())\nmedian_satisfaction_level = hr['satisfaction_level'].median()\nmedian_last_evaluation = hr['last_evaluation'].median()\nhr['satisfaction_level'].fillna(median_satisfaction_level, inplace = True)\nhr['last_evaluation'].fillna(median_last_evaluation, inplace = True)\nhr['sales'].fillna('KOSONG', inplace = True)\nhr['salary'].fillna('KOSONG', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nmedian_satisfaction_level = hr['satisfaction_level'].median()\nmedian_last_evaluation = hr['last_evaluation'].median()\nhr['satisfaction_level'].fillna(median_satisfaction_level, inplace = True)\nhr['last_evaluation'].fillna(median_last_evaluation, inplace = True)\nhr['sales'].fillna('KOSONG', inplace = True)\nhr['salary'].fillna('KOSONG', inplace = True)\ndf_sales = pd.get_dummies(hr['sales'])\ndf_salary = pd.get_dummies(hr['salary'])\nhr_new = pd.concat([hr, df_sales, df_salary], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nmedian_satisfaction_level = hr['satisfaction_level'].median()\nmedian_last_evaluation = hr['last_evaluation'].median()\nhr['satisfaction_level'].fillna(median_satisfaction_level, inplace = True)\nhr['last_evaluation'].fillna(median_last_evaluation, inplace = True)\nhr['sales'].fillna('KOSONG', inplace = True)\nhr['salary'].fillna('KOSONG', inplace = True)\ndf_sales = pd.get_dummies(hr['sales'])\ndf_salary = pd.get_dummies(hr['salary'])\nhr_new = pd.concat([hr, df_sales, df_salary], axis=1)\nhr_new = hr_new.drop(['sales','salary'], axis=1)\nfrom sklearn.preprocessing import StandardScaler\nscaled_features = StandardScaler().fit_transform(hr_new.values)\nscaled_features_hr = pd.DataFrame(scaled_features, index=hr_new.index, columns=hr_new.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nmedian_satisfaction_level = hr['satisfaction_level'].median()\nmedian_last_evaluation = hr['last_evaluation'].median()\nhr['satisfaction_level'].fillna(median_satisfaction_level, inplace = True)\nhr['last_evaluation'].fillna(median_last_evaluation, inplace = True)\nhr['sales'].fillna('KOSONG', inplace = True)\nhr['salary'].fillna('KOSONG', inplace = True)\ndf_sales = pd.get_dummies(hr['sales'])\ndf_salary = pd.get_dummies(hr['salary'])\nhr_new = pd.concat([hr, df_sales, df_salary], axis=1)\nhr_new = hr_new.drop(['sales','salary'], axis=1)\nfrom sklearn.preprocessing import StandardScaler\nscaled_features = StandardScaler().fit_transform(hr_new.values)\nscaled_features_hr = pd.DataFrame(scaled_features, index=hr_new.index, columns=hr_new.columns)\n\nfrom sklearn import preprocessing\nfeature_cols = ['satisfaction_level','last_evaluation','number_project',\n                'average_montly_hours','time_spend_company','Work_accident',\n                'promotion_last_5years', 'IT', 'KOSONG', 'RandD', 'accounting',\n                'hr', 'management', 'marketing', 'product_mng', 'support',\n                'technical', 'KOSONG', 'high', 'low', 'medium']\nX = scaled_features_hr[feature_cols]\ny = scaled_features_hr.left\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\nlab_enc = preprocessing.LabelEncoder()\ntraining_scores_encoded = lab_enc.fit_transform(y_train)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nmodel = LogisticRegression()\nmodel.fit(X_train, training_scores_encoded)\npredicted_classes = model.predict(X_train)\naccuracy = accuracy_score(training_scores_encoded.flatten(),predicted_classes)\nprint(model.coef_)\nprint(accuracy)\ntest_scores_encoded = lab_enc.fit_transform(y_test)\nprint(model.score(X_test, test_scores_encoded))\nlab_enc = preprocessing.LabelEncoder()\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier(n_estimators=100)\nclf.fit(X_train,training_scores_encoded)\ny_pred=clf.predict(X_test)\nfrom sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(test_scores_encoded, y_pred))\n\nfrom sklearn.ensemble import AdaBoostClassifier\nabc = AdaBoostClassifier(n_estimators=50,\n                         learning_rate=1)\nmodel = abc.fit(X_train, training_scores_encoded)\ny_pred = model.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(test_scores_encoded, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nmedian_satisfaction_level = hr['satisfaction_level'].median()\nmedian_last_evaluation = hr['last_evaluation'].median()\nhr['satisfaction_level'].fillna(median_satisfaction_level, inplace = True)\nhr['last_evaluation'].fillna(median_last_evaluation, inplace = True)\nhr['sales'].fillna('KOSONG', inplace = True)\nhr['salary'].fillna('KOSONG', inplace = True)\ndf_sales = pd.get_dummies(hr['sales'])\ndf_salary = pd.get_dummies(hr['salary'])\nhr_new = pd.concat([hr, df_sales, df_salary], axis=1)\nhr_new = hr_new.drop(['sales','salary'], axis=1)\nfrom sklearn.preprocessing import StandardScaler\nscaled_features = StandardScaler().fit_transform(hr_new.values)\nscaled_features_hr = pd.DataFrame(scaled_features, index=hr_new.index, columns=hr_new.columns)\n\nfrom sklearn import preprocessing\nfeature_cols = ['satisfaction_level','last_evaluation','number_project',\n                'average_montly_hours','time_spend_company','Work_accident',\n                'promotion_last_5years', 'IT', 'KOSONG', 'RandD', 'accounting',\n                'hr', 'management', 'marketing', 'product_mng', 'support',\n                'technical', 'KOSONG', 'high', 'low', 'medium']\nX = scaled_features_hr[feature_cols]\ny = scaled_features_hr.left\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\nlab_enc = preprocessing.LabelEncoder()\ntraining_scores_encoded = lab_enc.fit_transform(y_train)\ntest_scores_encoded = lab_enc.fit_transform(y_test)\nfrom sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier(n_estimators=100)\nclf.fit(X_train,training_scores_encoded)\nestimator = clf.estimators_[5]\nfrom sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO\nimport pydotplus\nfrom IPython.display import Image\ndot_data = StringIO()\nexport_graphviz(estimator, out_file=dot_data,\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nmedian_satisfaction_level = hr['satisfaction_level'].median()\nmedian_last_evaluation = hr['last_evaluation'].median()\nhr['satisfaction_level'].fillna(median_satisfaction_level, inplace = True)\nhr['last_evaluation'].fillna(median_last_evaluation, inplace = True)\nhr['sales'].fillna('KOSONG', inplace = True)\nhr['salary'].fillna('KOSONG', inplace = True)\ndf_sales = pd.get_dummies(hr['sales'])\ndf_salary = pd.get_dummies(hr['salary'])\nhr_new = pd.concat([hr, df_sales, df_salary], axis=1)\nhr_new = hr_new.drop(['sales','salary'], axis=1)\nfrom sklearn.preprocessing import StandardScaler\nscaled_features = StandardScaler().fit_transform(hr_new.values)\nscaled_features_hr = pd.DataFrame(scaled_features, index=hr_new.index, columns=hr_new.columns)\n\nfrom sklearn import preprocessing\nfeature_cols = ['satisfaction_level','last_evaluation','number_project',\n                'average_montly_hours','time_spend_company','Work_accident',\n                'promotion_last_5years', 'IT', 'KOSONG', 'RandD', 'accounting',\n                'hr', 'management', 'marketing', 'product_mng', 'support',\n                'technical', 'KOSONG', 'high', 'low', 'medium']\nX = scaled_features_hr[feature_cols]\ny = scaled_features_hr.left\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\nlab_enc = preprocessing.LabelEncoder()\ntraining_scores_encoded = lab_enc.fit_transform(y_train)\ntest_scores_encoded = lab_enc.fit_transform(y_test)\nfrom sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier(n_estimators=100)\nclf.fit(X_train,training_scores_encoded)\nfeat_importances = pd.Series(clf.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nhr = pd.read_csv(\"../input/lab2-hr-data/hr_data.csv\")\nmedian_satisfaction_level = hr['satisfaction_level'].median()\nmedian_last_evaluation = hr['last_evaluation'].median()\nhr['satisfaction_level'].fillna(median_satisfaction_level, inplace = True)\nhr['last_evaluation'].fillna(median_last_evaluation, inplace = True)\nhr['sales'].fillna('KOSONG', inplace = True)\nhr['salary'].fillna('KOSONG', inplace = True)\ndf_sales = pd.get_dummies(hr['sales'])\ndf_salary = pd.get_dummies(hr['salary'])\nhr_new = pd.concat([hr, df_sales, df_salary], axis=1)\nhr_new = hr_new.drop(['sales','salary'], axis=1)\nfrom sklearn.preprocessing import StandardScaler\nscaled_features = StandardScaler().fit_transform(hr_new.values)\nscaled_features_hr = pd.DataFrame(scaled_features, index=hr_new.index, columns=hr_new.columns)\n\nfrom sklearn import preprocessing\nimport numpy as np\nfeature_cols = ['satisfaction_level','last_evaluation','number_project',\n                'average_montly_hours','time_spend_company','Work_accident',\n                'promotion_last_5years', 'IT', 'KOSONG', 'RandD', 'accounting',\n                'hr', 'management', 'marketing', 'product_mng', 'support',\n                'technical', 'KOSONG', 'high', 'low', 'medium']\nX = scaled_features_hr[feature_cols]\ny = scaled_features_hr.left\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\nlab_enc = preprocessing.LabelEncoder()\ntraining_scores_encoded = lab_enc.fit_transform(y_train)\ntest_scores_encoded = lab_enc.fit_transform(y_test)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import linear_model\nlogistic = linear_model.LogisticRegression()\n# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\nclf = GridSearchCV(logistic, hyperparameters, cv=5)\nclf.fit(X_train, training_scores_encoded)\nprint(\"Tuned Logistic Regression Parameters: {}\".format(clf.best_params_)) \nprint(\"Best score is {}\".format(clf.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bagian 2 data train.csv, macro.csv, test.csv**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('../input/lab2-russian-house-price/train.csv')\nprint(train['timestamp'].describe())\ntrain = pd.read_csv('../input/lab2-russian-house-price/train.csv', parse_dates=['timestamp'])\nprint(train['timestamp'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('../input/lab2-russian-house-price/train.csv', parse_dates=['timestamp'])\ntest = pd.read_csv('../input/lab2-russian-house-price/test.csv', parse_dates=['timestamp'])\ndf = pd.concat([train, test])\nmacro = pd.read_csv('../input/lab2-russian-house-price/macro.csv', parse_dates=['timestamp'])\ndf = pd.merge_ordered(df, macro, on=['timestamp'])\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('../input/lab2-russian-house-price/train.csv', parse_dates=['timestamp'])\ntest = pd.read_csv('../input/lab2-russian-house-price/test.csv', parse_dates=['timestamp'])\ndf = pd.concat([train, test])\nmacro = pd.read_csv('../input/lab2-russian-house-price/macro.csv', parse_dates=['timestamp'])\ndf = pd.merge_ordered(df, macro, on=['timestamp'])\n\nfrom sklearn.model_selection import train_test_split\ntraining = df\nX = training.drop(['price_doc'], axis=1)\ny = df['price_doc']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('../input/lab2-russian-house-price/train.csv', parse_dates=['timestamp'])\ntest = pd.read_csv('../input/lab2-russian-house-price/test.csv', parse_dates=['timestamp'])\ndf = pd.concat([train, test])\nmacro = pd.read_csv('../input/lab2-russian-house-price/macro.csv', parse_dates=['timestamp'])\ndf = pd.merge_ordered(df, macro, on=['timestamp'])\n\nimport numpy as np\nfor item in df:\n    if(df[item].dtype == np.int64 or df[item].dtype == np.float64):\n        if df[item].isnull().any():\n            df[item] = df[item].fillna(df[item].median())\n    else:\n        df[item] = df[item].fillna('KOSONG')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('../input/lab2-russian-house-price/train.csv', parse_dates=['timestamp'])\ntest = pd.read_csv('../input/lab2-russian-house-price/test.csv', parse_dates=['timestamp'])\ndf = pd.concat([train, test])\nmacro = pd.read_csv('../input/lab2-russian-house-price/macro.csv', parse_dates=['timestamp'])\ndf = pd.merge_ordered(df, macro, on=['timestamp'])\n\nfor item in df:\n    if(df[item].dtype == np.int64 or df[item].dtype == np.float64):\n        if df[item].isnull().any():\n            df[item] = df[item].fillna(df[item].median())\n    else:\n        df[item] = df[item].fillna('KOSONG')\n\nnumerical_features = list()\ncategorical_features = list()\nfor item in df:\n    if(df[item].dtype == np.int64 or df[item].dtype == np.float64):\n        numerical_features.append(item)\n    else:\n        categorical_features.append(item)\nnumerical = pd.get_dummies(df[numerical_features])\ncategorical = pd.get_dummies(df[categorical_features])\n\nfrom sklearn.preprocessing import StandardScaler\nscaled_numerical = StandardScaler().fit_transform(numerical.values)\nscaled_numerical = pd.DataFrame(scaled_numerical, index=numerical.index, columns=numerical.columns)\nscaled_numerical_noPrice = scaled_numerical.drop(['price_doc'], axis = 1)\nfrom sklearn.model_selection import train_test_split\nX = np.hstack((scaled_numerical_noPrice,categorical))\ny = scaled_numerical['price_doc']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n\nfrom sklearn.model_selection import train_test_split\ntraining = scaled_df\nX = training.drop(['price_doc'], axis = 1)\ny = scaled_df['price_doc']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('../input/lab2-russian-house-price/train.csv', parse_dates=['timestamp'])\ntest = pd.read_csv('../input/lab2-russian-house-price/test.csv', parse_dates=['timestamp'])\ntrainTest = pd.concat([train, test])\nmacro = pd.read_csv('../input/lab2-russian-house-price/macro.csv', parse_dates=['timestamp'])\ndf = pd.merge_ordered(trainTest, macro, on=['timestamp'])\n\nfor item in df:\n    if(df[item].dtype == np.int64 or df[item].dtype == np.float64):\n        df[item] = df[item].fillna(df[item].median())\n    else:\n        df[item] = df[item].fillna('KOSONG')\n\nnumerical_features = list()\ncategorical_features = list()\nfor item in df:\n    if(df[item].dtype == np.int64 or df[item].dtype == np.float64):\n        numerical_features.append(item)\n    else:\n        categorical_features.append(item)\nnumerical = pd.get_dummies(df[numerical_features])\ncategorical = pd.get_dummies(df[categorical_features])\ncategorical = categorical.drop(['timestamp'], axis = 1)\n\nfrom sklearn.preprocessing import StandardScaler\nscaled_numerical = StandardScaler().fit_transform(numerical.values)\nscaled_numerical = pd.DataFrame(scaled_numerical, index=numerical.index, columns=numerical.columns)\nscaled_numerical_noPrice = scaled_numerical.drop(['price_doc'], axis = 1)\n\nfrom sklearn.model_selection import train_test_split\nX = np.hstack((scaled_numerical_noPrice,categorical))\ny = scaled_numerical['price_doc']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\ndef smape(A, F):\n    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\nreg = LinearRegression()\nreg.fit(X_train, y_train)\nprint('Linear Regression score: %f' % reg.score(X_test, y_test))\ny_pred = reg.predict(X_test)\nprint('RMSE score: %f' % mean_squared_error(y_test, y_pred))\nprint('MAE score: %f' % mean_absolute_error(y_test, y_pred))\nprint('SMAPE score: %f' % smape(y_test, y_pred))\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nlasso = Lasso()\nlasso.fit(X_train, y_train)\nprint('Lasso score: %f' % lasso.score(X_test, y_test))\ny_pred = lasso.predict(X_test)\nprint('RMSE score: %f' % mean_squared_error(y_test, y_pred))\nprint('MAE score: %f' % mean_absolute_error(y_test, y_pred))\ndef smape(A, F):\n    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\nprint('SMAPE score: %f' % smape(y_test, y_pred))\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\ndef smape(A, F):\n    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\nridge = Ridge()\nridge.fit(X_train, y_train)\nprint('Ridge score: %f' % ridge.score(X_test, y_test))\ny_pred = ridge.predict(X_test)\nprint('RMSE score: %f' % mean_squared_error(y_test, y_pred))\nprint('MAE score: %f' % mean_absolute_error(y_test, y_pred))\nprint('SMAPE score: %f' % smape(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('../input/lab2-russian-house-price/train.csv', parse_dates=['timestamp'])\ntest = pd.read_csv('../input/lab2-russian-house-price/test.csv', parse_dates=['timestamp'])\ntrainTest = pd.concat([train, test])\nmacro = pd.read_csv('../input/lab2-russian-house-price/macro.csv', parse_dates=['timestamp'])\ndf = pd.merge_ordered(trainTest, macro, on=['timestamp'])\ndf = df.drop(['oil_urals'], axis = 1)\n\nfor item in df:\n    if(df[item].dtype == np.int64 or df[item].dtype == np.float64):\n        df[item] = df[item].fillna(df[item].median())\n    else:\n        df[item] = df[item].fillna('KOSONG')\n\nnumerical_features = list()\ncategorical_features = list()\nfor item in df:\n    if(df[item].dtype == np.int64 or df[item].dtype == np.float64):\n        numerical_features.append(item)\n    else:\n        categorical_features.append(item)\nnumerical = pd.get_dummies(df[numerical_features])\ncategorical = pd.get_dummies(df[categorical_features])\ncategorical = categorical.drop(['timestamp'], axis = 1)\n\nfrom sklearn.preprocessing import StandardScaler\nscaled_numerical = StandardScaler().fit_transform(numerical.values)\nscaled_numerical = pd.DataFrame(scaled_numerical, index=numerical.index, columns=numerical.columns)\nscaled_numerical_noPrice = scaled_numerical.drop(['price_doc'], axis = 1)\n\nfrom sklearn.model_selection import train_test_split\nX = np.hstack((scaled_numerical_noPrice,categorical))\ny = scaled_numerical['price_doc']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\ndef smape(A, F):\n    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\nreg = LinearRegression()\nreg.fit(X_train, y_train)\nprint('Linear Regression score: %f' % reg.score(X_test, y_test))\ny_pred = reg.predict(X_test)\nprint('RMSE score: %f' % mean_squared_error(y_test, y_pred))\nprint('MAE score: %f' % mean_absolute_error(y_test, y_pred))\nprint('SMAPE score: %f' % smape(y_test, y_pred))\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nlasso = Lasso()\nlasso.fit(X_train, y_train)\nprint('Lasso score: %f' % lasso.score(X_test, y_test))\ny_pred = lasso.predict(X_test)\nprint('RMSE score: %f' % mean_squared_error(y_test, y_pred))\nprint('MAE score: %f' % mean_absolute_error(y_test, y_pred))\ndef smape(A, F):\n    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\nprint('SMAPE score: %f' % smape(y_test, y_pred))\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\ndef smape(A, F):\n    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\nridge = Ridge()\nridge.fit(X_train, y_train)\nprint('Ridge score: %f' % ridge.score(X_test, y_test))\ny_pred = ridge.predict(X_test)\nprint('RMSE score: %f' % mean_squared_error(y_test, y_pred))\nprint('MAE score: %f' % mean_absolute_error(y_test, y_pred))\nprint('SMAPE score: %f' % smape(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.5,random_state=0)\n\nfrom sklearn import preprocessing\nlab_enc = preprocessing.LabelEncoder()\ntraining_scores_encoded = lab_enc.fit_transform(y_train)\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression()\n# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\nclf = GridSearchCV(logistic, hyperparameters, cv=5)\nclf.fit(X_train, training_scores_encoded)\nprint(\"Tuned Logistic Regression Parameters: {}\".format(clf.best_params_)) \nprint(\"Best score is {}\".format(clf.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bagian 3 data movies_metadata.csv**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nmovies = pd.read_csv('../input/lab2-movies-metadata/movies_metadata.csv')\nprint(movies.shape)\ndf = movies.drop(movies.index[19730])\ndf = df.drop(df.index[29502])\ndf = df.drop(df.index[35585])\nprint(df.shape)\n\nrank = movies.isnull().sum()\nrank.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attr = ['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count', 'title']\nnew_df = movies[attr]\nprint(new_df['vote_count'].describe())\nprint(new_df['vote_count'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = new_df.sample(n=10000, random_state=1)\nprint(sample_df['vote_count'].describe())\nprint(sample_df['vote_count'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = new_df.sample(n=10000, random_state=1)\nnew_sample_df = sample_df.drop(sample_df[sample_df.vote_count < 30].index)\nprint(new_sample_df.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = new_df.sample(n=10000, random_state=1)\nnew_sample_df = sample_df.drop(sample_df[sample_df.vote_count < 30].index)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nattr_transform = ['popularity', 'revenue', 'vote_count']\ndata_transform = new_sample_df[attr_transform]\nscaled_data = scaler.fit_transform(data_transform.values)\nscaled_df = pd.DataFrame(scaled_data, index=data_transform.index, columns=data_transform.columns)\nscaled_df = scaled_df.dropna()\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport pylab as pl\nimport numpy as np\nimport matplotlib.pyplot as plt\nY = np.array(scaled_df[['popularity', 'revenue']])\nX = np.array(scaled_df[['vote_count', 'revenue']])\nNc = range(1, 20)\nkmeans = [KMeans(n_clusters=i) for i in Nc]\nscore = [kmeans[i].fit(Y).score(Y) for i in range(len(kmeans))]\n\npl.plot(Nc,score)\npl.xlabel('Number of Clusters')\npl.ylabel('Score')\npl.title('Elbow Curve')\npl.show()\n\npca = PCA(n_components=1).fit(Y)\npca_d = pca.transform(Y)\npca_c = pca.transform(X)\nkmeans=KMeans(n_clusters=5)\nkmeansoutput=kmeans.fit(Y)\nkmeansoutput\npl.figure('5 Cluster K-Means')\npl.scatter(pca_c[:, 0], pca_d[:, 0], c=kmeansoutput.labels_)\npl.xlabel('vote_count and revenue')\npl.ylabel('popularity and revenue')\npl.title('5 Cluster K-Means')\npl.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}